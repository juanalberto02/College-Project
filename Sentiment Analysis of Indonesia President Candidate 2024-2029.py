# -*- coding: utf-8 -*-
"""NLP_SD-A1_162112133011_162112133016_162112133027_162112133032_162112133034.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ARE3S8XfNGWXHlDScjaUYYVKlWcMscqN

# **1. LIBRARIES**
"""

# Import necessary libraries
from sklearn.model_selection import train_test_split
from transformers import BertTokenizer, BertForSequenceClassification, AdamW, TFBertForSequenceClassification, BertConfig
from tensorflow.keras.optimizers import Adam
from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset
import pandas as pd
from google.colab import drive
from tensorflow.keras.layers import Dropout
from tensorflow.keras.regularizers import l2
import re
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import numpy as np
from sklearn.metrics import classification_report
import seaborn as sns
import torch
from tqdm import tqdm
from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping
from tensorflow.keras.optimizers import Adam
import tensorflow as tf

"""# **2. DATA PREPROCESSING**"""

# DRIVE
# drive.mount('/content/drive')
# df = pd.read_csv('/content/drive/MyDrive/dataset_UAS.csv')


df = pd.read_csv('dataset_UAS.csv')
df = df.rename(columns={'sentimen (positif 1, netral 2, negatif3)': 'sentimen'})
df

df.dropna(subset=['full_text'], inplace=True)
df.reset_index(drop=True, inplace=True)
df

df['sentimen'].unique()

# Membuat dictionary untuk mapping nilai sentimen ke angka
# 0 = positif, 1 = netral, 2 = negatif
mapping = {'positif': 0, 'netral': 1, 'negatif': 2,
           'positif ': 0, 'netral ': 1, 'negatif ':2,
           'posiitf': 0, 'positig': 0, ' netral': 1,
           'Netral': 1 , 'Positif': 0, 'Negatif': 2,
           '1':0,'2':1,'3':2}

df['sentimen'] = df['sentimen'].astype(str)

# Mengubah nilai pada kolom "sentimen" menggunakan map()
df['sentimen'] = df['sentimen'].map(mapping)
df

df['sentimen'].unique()

data_train = df[
    df['full_text'].notna() &
    df['sentimen'].notna() &
    df['anies'].notna() &
    df['ganjar'].notna() &
    df['prabowo'].notna()
]
data_train.reset_index(drop=True, inplace=True)
data_train

data_test = df[
    df['full_text'].notna() &
    df['sentimen'].isna() &
    df['anies'].isna() &
    df['ganjar'].isna() &
    df['prabowo'].isna()
]
data_test = data_test["full_text"]
data_test.reset_index(drop=True, inplace=True)
data_test

def prepro(text):
    text = text.lower() # ubah jadi lower case
    text = re.sub(r"http\S+|www.\S+", ' ', text) # Menghapus url
    text = re.sub(r'@(\w+)', r'\1', text) #hapus @
    text = re.sub(r'#(\w+)', r'\1', text) #hapus #
    text = re.sub(r'_', ' ', text) # ubah underscore menjadi spasi
    text = re.sub(r'[^\w\s?!.,]', ' ', text) # hapus tanda baca selain "?", "!", "," , dan "."
    text = re.sub(r' +', ' ', text).strip() # Beresin spasi dobel
    return text

data_train["clean_text"] = data_train["full_text"].map(prepro)
data_train

# kamus_df = pd.read_csv('/content/drive/MyDrive/singkatan+alay.csv')
kamus_df = pd.read_csv('kamus_alay.csv')
kamus = dict(zip(kamus_df['slang'], kamus_df['formal']))

def ganti_kata_alay(teks):
    kata_kata = teks.split()
    teks_hasil = " ".join([kamus.get(kata, kata) for kata in kata_kata])
    return teks_hasil

data_train['clean_text'] = data_train['clean_text'].apply(ganti_kata_alay)
data_train

data_train.info()

kolom = ["sentimen", "anies", "ganjar", "prabowo"]

for i in kolom:
    data_train[i] = data_train[i].astype(int)

data_train.info()

data_train

"""## **2.1 Train Data Splitting**"""

# 1. Split the dataset
X_train, X_val, y_train, y_val = train_test_split(data_train['clean_text'], data_train['sentimen'], test_size=0.20, random_state=42)

"""# **3. DATA MODELLING**

## **3.2 indolem/indobert-base-uncased**
"""

# Load IndoBERT model and tokenizer
indolem2 = 'indolem/indobert-base-uncased'
tokenizer2 = BertTokenizer.from_pretrained(indolem2)

# Adjust the model configuration
config2 = BertConfig.from_pretrained(indolem2, num_labels=3, hidden_dropout_prob=0.4, attention_probs_dropout_prob=0.4)

# Load the model with dropout
model2 = TFBertForSequenceClassification.from_pretrained(indolem2, config=config2, from_pt=True)

# Tokenize the input (X_train and X_val)
train_encodings2 = tokenizer2(X_train.tolist(), truncation=True, padding=True, max_length=128)
val_encodings2 = tokenizer2(X_val.tolist(), truncation=True, padding=True, max_length=128)

# Convert to TensorFlow datasets
train_dataset2 = tf.data.Dataset.from_tensor_slices((dict(train_encodings2), y_train))
val_dataset2 = tf.data.Dataset.from_tensor_slices((dict(val_encodings2), y_val))

# Custom loss function
loss_function2 = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)

class CustomCallback(tf.keras.callbacks.Callback):
    def __init__(self, target_train_accuracy=0.78, target_val_accuracy=0.78):
        super(CustomCallback, self).__init__()
        self.target_train_accuracy = target_train_accuracy
        self.target_val_accuracy = target_val_accuracy

    def on_epoch_end(self, epoch, logs=None):
        train_accuracy = logs.get('accuracy')
        val_accuracy = logs.get('val_accuracy')
        if train_accuracy is not None and val_accuracy is not None:
            if train_accuracy >= self.target_train_accuracy and val_accuracy >= self.target_val_accuracy:
                print(f"\nTraining Accuracy: {train_accuracy:.2f}, Validation Accuracy: {val_accuracy:.2f}")
                print(f"Reached target accuracies: training ({self.target_train_accuracy * 100}%) and validation ({self.target_val_accuracy * 100}%). Stopping training.")
                self.model.stop_training = True

# Initialize your custom callback with desired accuracies
custom_callback2 = CustomCallback(target_train_accuracy=0.78, target_val_accuracy=0.78)

# Implement callbacks
reduce_lr2 = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001)

# Adam Optimizer
optimizer2 = Adam(learning_rate=3e-5)

# Compile the model
model2.compile(optimizer=optimizer2, loss=loss_function2, metrics=['accuracy'])

# Verify data types and shapes
print("Training set types and shapes:")
for x, y in train_dataset2.take(1):
    print("Input:", x['input_ids'].dtype, x['input_ids'].shape)
    print("Label:", y.dtype, y.shape)

print("\nValidation set types and shapes:")
for x, y in val_dataset2.take(1):
    print("Input:", x['input_ids'].dtype, x['input_ids'].shape)
    print("Label:", y.dtype, y.shape)

# Reduce the number of epochs
history2 = model2.fit(train_dataset2.shuffle(2000).batch(32), epochs=20,
                    validation_data=val_dataset2.batch(32), callbacks=[reduce_lr2, custom_callback2])

# Evaluate the model to check if the accuracy criteria is met
train_accuracy2 = history2.history['accuracy'][-1]
val_accuracy2 = history2.history['val_accuracy'][-1]

print(f"Training Accuracy: {train_accuracy2*100:.2f}%")
print(f"Validation Accuracy: {val_accuracy2*100:.2f}%")

# 1. Classification Report
y_val_pred = model2.predict(val_dataset2.batch(32)).logits
y_val_pred_class = tf.argmax(y_val_pred, axis=1).numpy()
print(classification_report(y_val, y_val_pred_class, target_names=['positif', 'netral', 'negatif']))

# 2. Prediksi Sentimen pada data_test
data_test_preprocessed = data_test.map(prepro).apply(ganti_kata_alay)
test_encodings = tokenizer2(data_test_preprocessed.tolist(), truncation=True, padding=True, max_length=128)
test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings)))
test_predictions = model2.predict(test_dataset.batch(32)).logits
test_pred_class = tf.argmax(test_predictions, axis=1).numpy()

# Mengubah data_test dan test_pred_class menjadi DataFrame
df_test_predictions = pd.DataFrame({
    'text': data_test_preprocessed,
    'predicted_sentiment': test_pred_class
})

# Mengganti nilai numerik pada 'predicted_sentiment' dengan label yang sesuai
sentiment_labels = {0: 'positif', 1: 'netral', 2: 'negatif'}
df_test_predictions['predicted_sentiment'] = df_test_predictions['predicted_sentiment'].map(sentiment_labels)

df_test_predictions

# Fungsi untuk membuat pie chart
def plot_pie_chart(values, labels, title):
    plt.figure(figsize=(8, 6))
    plt.pie(values, labels=labels, autopct='%1.1f%%', startangle=140)
    plt.title(title)
    plt.show()

# Menghitung jumlah masing-masing sentimen pada data_test
sentimen_counts_test = df_test_predictions['predicted_sentiment'].value_counts().sort_index()
plot_pie_chart(sentimen_counts_test, ['positif', 'netral', 'negatif'], "Distribusi Sentimen pada Data Test (indolem)")

# Fungsi untuk membuat pie chart
def plot_pie_chart(values, labels, title):
    plt.figure(figsize=(8, 6))
    plt.pie(values, labels=labels, autopct='%1.1f%%', startangle=140)
    plt.title(title)
    plt.show()

# Menghitung jumlah masing-masing sentimen pada data_train
sentimen_counts_train = data_train['sentimen'].value_counts().sort_index()
plot_pie_chart(sentimen_counts_train, ['positif', 'netral', 'negatif'], "Distribusi Sentimen pada Data Train (indolem)")

# Menghitung jumlah masing-masing sentimen pada data_train + prediksi dari data_test
combined_sentiments = list(data_train['sentimen']) + list(test_pred_class)
combined_sentiments_counts = pd.Series(combined_sentiments).value_counts().sort_index()
plot_pie_chart(combined_sentiments_counts, ['positif', 'netral', 'negatif'], "Distribusi Sentimen pada Data Train + Prediksi Data Test")

# Visualisasi Distribusi Sentimen Berdasarkan Tokoh Politik
def plot_pie_for_politician(data, politician, title):
    sentiment_counts = data[data[politician] == 1]['sentimen'].value_counts().sort_index()
    plot_pie_chart(sentiment_counts, ['positif', 'netral', 'negatif'], title)

plot_pie_for_politician(data_train, 'anies', "Distribusi Sentimen - Anies")

plot_pie_for_politician(data_train, 'ganjar', "Distribusi Sentimen - Ganjar")

plot_pie_for_politician(data_train, 'prabowo', "Distribusi Sentimen - Prabowo")

"""## 3.3 Indobert tweet"""

# 2. Load IndoBERT model and tokenizer
model_name = 'indolem/indobertweet-base-uncased'
tokenizer = BertTokenizer.from_pretrained(model_name)
config = BertConfig.from_pretrained(model_name, num_labels=3)
model = TFBertForSequenceClassification.from_pretrained(model_name, config=config, from_pt=True)

# Tokenize the input (X_train and X_val)
train_encodings = tokenizer(X_train.tolist(), truncation=True, padding=True, max_length=128)
val_encodings = tokenizer(X_val.tolist(), truncation=True, padding=True, max_length=128)

# Convert to TensorFlow datasets
train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), y_train))
val_dataset = tf.data.Dataset.from_tensor_slices((dict(val_encodings), y_val))

# Custom loss function (if needed)
loss_function = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)

# 3. Implement callbacks
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001)
early_stopping = EarlyStopping(monitor='val_loss', patience=10)

# 4. Train the model
optimizer = Adam(learning_rate=5e-5)
model.compile(optimizer=optimizer, loss=loss_function, metrics=['accuracy'])

# Verify data types and shapes
print("Training set types and shapes:")
for x, y in train_dataset.take(1):
    print("Input:", x['input_ids'].dtype, x['input_ids'].shape)
    print("Label:", y.dtype, y.shape)

print("\nValidation set types and shapes:")
for x, y in val_dataset.take(1):
    print("Input:", x['input_ids'].dtype, x['input_ids'].shape)
    print("Label:", y.dtype, y.shape)

# Training the model
history = model.fit(train_dataset.shuffle(1000).batch(32), epochs=20,
                    validation_data=val_dataset.batch(32), callbacks=[reduce_lr, early_stopping])

# Evaluate the model to check if the accuracy criteria is met
train_accuracy = history.history['accuracy'][-1]
val_accuracy = history.history['val_accuracy'][-1]

print(f"Training Accuracy: {train_accuracy*100:.2f}%")
print(f"Validation Accuracy: {val_accuracy*100:.2f}%")

# 1. Classification Report
y_val_pred = model.predict(val_dataset.batch(32)).logits
y_val_pred_class = tf.argmax(y_val_pred, axis=1).numpy()
print(classification_report(y_val, y_val_pred_class, target_names=['positif', 'netral', 'negatif']))

# 2. Prediksi Sentimen pada data_test
data_test_preprocessed = data_test.map(prepro).apply(ganti_kata_alay)
test_encodings = tokenizer(data_test_preprocessed.tolist(), truncation=True, padding=True, max_length=128)
test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings)))
test_predictions = model.predict(test_dataset.batch(32)).logits
test_pred_class = tf.argmax(test_predictions, axis=1).numpy()

# Mengubah data_test dan test_pred_class menjadi DataFrame
df_test_predictions = pd.DataFrame({
    'text': data_test_preprocessed,
    'predicted_sentiment': test_pred_class
})

# Mengganti nilai numerik pada 'predicted_sentiment' dengan label yang sesuai
sentiment_labels = {0: 'positif', 1: 'netral', 2: 'negatif'}
df_test_predictions['predicted_sentiment'] = df_test_predictions['predicted_sentiment'].map(sentiment_labels)

df_test_predictions



# Fungsi untuk membuat pie chart
def plot_pie_chart(values, labels, title):
    plt.figure(figsize=(8, 6))
    plt.pie(values, labels=labels, autopct='%1.1f%%', startangle=140)
    plt.title(title)
    plt.show()

# Menghitung jumlah masing-masing sentimen pada data_train
sentimen_counts_test = df_test_predictions['predicted_sentiment'].value_counts().sort_index()
plot_pie_chart(sentimen_counts_test, ['positif', 'netral', 'negatif'], "Distribusi Sentimen pada Data Test (indobertweet)")



"""Word Cloud"""

from wordcloud import WordCloud
import matplotlib.pyplot as plt

def generate_wordcloud(text, title):
    # Generate the word cloud
    wordcloud = WordCloud(width=800, height=400, random_state=21, max_font_size=110, background_color='white').generate(text)

    # Plot the WordCloud image
    plt.figure(figsize=(10, 7))
    plt.imshow(wordcloud, interpolation="bilinear")
    plt.axis('off')
    plt.title(title)
    plt.show()

# Example: Assuming you have a column named 'text' in your DataFrame
text_data = ' '.join(df_test_predictions['text'].values)
generate_wordcloud(text_data, "Word Cloud from Test Data")

# Fungsi untuk membuat pie chart
def plot_pie_chart(values, labels, title):
    plt.figure(figsize=(8, 6))
    plt.pie(values, labels=labels, autopct='%1.1f%%', startangle=140)
    plt.title(title)
    plt.show()

# Menghitung jumlah masing-masing sentimen pada data_train
sentimen_counts_train = data_train['sentimen'].value_counts().sort_index()
plot_pie_chart(sentimen_counts_train, ['positif', 'netral', 'negatif'], "Distribusi Sentimen pada Data Train (indobertweet)")

# Menghitung jumlah masing-masing sentimen pada data_train + prediksi dari data_test
combined_sentiments = list(data_train['sentimen']) + list(test_pred_class)
combined_sentiments_counts = pd.Series(combined_sentiments).value_counts().sort_index()
plot_pie_chart(combined_sentiments_counts, ['positif', 'netral', 'negatif'], "Distribusi Sentimen pada Data Train + Prediksi Data Test")

# Visualisasi Distribusi Sentimen Berdasarkan Tokoh Politik
def plot_pie_for_politician(data, politician, title):
    sentiment_counts = data[data[politician] == 1]['sentimen'].value_counts().sort_index()
    plot_pie_chart(sentiment_counts, ['positif', 'netral', 'negatif'], title)

plot_pie_for_politician(data_train, 'anies', "Distribusi Sentimen - Anies")

plot_pie_for_politician(data_train, 'ganjar', "Distribusi Sentimen - Ganjar")

plot_pie_for_politician(data_train, 'prabowo', "Distribusi Sentimen - Prabowo")

"""## 3.3 XLNet"""

!pip install transformers
from transformers import XLNetTokenizer, TFXLNetForSequenceClassification, XLNetConfig

import tensorflow as tf
from tensorflow.keras.callbacks import ModelCheckpoint
model_name = 'xlnet-base-cased'  # Replace with the appropriate XLNet model name if needed
tokenizer = XLNetTokenizer.from_pretrained(model_name)
config = XLNetConfig.from_pretrained(model_name, num_labels=3)  # Adjust num_labels if necessary
model = TFXLNetForSequenceClassification.from_pretrained(model_name, config=config)

# Tokenize the input (X_train and X_val)
train_encodings = tokenizer(X_train.tolist(), truncation=True, padding=True, max_length=64)
val_encodings = tokenizer(X_val.tolist(), truncation=True, padding=True, max_length=64)

# Convert to TensorFlow datasets
train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), y_train))
val_dataset = tf.data.Dataset.from_tensor_slices((dict(val_encodings), y_val))

# Custom loss function (if needed)
loss_function = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)

# Implement callbacks
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001)

# Train the model
optimizer = Adam(learning_rate=5e-5)
model.compile(optimizer=optimizer, loss=loss_function, metrics=['accuracy'])

# Verify data types and shapes (unchanged)
print("Training set types and shapes:")
for x, y in train_dataset.take(1):
    print("Input:", x['input_ids'].dtype, x['input_ids'].shape)
    print("Label:", y.dtype, y.shape)

print("\nValidation set types and shapes:")
for x, y in val_dataset.take(1):
    print("Input:", x['input_ids'].dtype, x['input_ids'].shape)
    print("Label:", y.dtype, y.shape)

# Training the model
history = model.fit(train_dataset.shuffle(1000).batch(32), epochs=5,
                    validation_data=val_dataset.batch(32), callbacks=[reduce_lr])



# Evaluate the model
train_accuracy = history.history['accuracy'][-1]
val_accuracy = history.history['val_accuracy'][-1]

print(f"Training Accuracy: {train_accuracy*100:.2f}%")
print(f"Validation Accuracy: {val_accuracy*100:.2f}%")

y_val_pred = model.predict(val_dataset.batch(32)).logits
y_val_pred_class = tf.argmax(y_val_pred, axis=1).numpy()
print(classification_report(y_val, y_val_pred_class, target_names=['positif', 'netral', 'negatif']))

from sklearn.metrics import classification_report
print(classification_report(y_val, y_val_pred_class, target_names=['positif', 'netral', 'negatif']))

# 2. Prediksi Sentimen pada data_test
data_test_preprocessed = data_test.map(prepro).apply(ganti_kata_alay)
test_encodings = tokenizer(data_test_preprocessed.tolist(), truncation=True, padding=True, max_length=64)
test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings)))
test_predictions = model.predict(test_dataset.batch(32)).logits
test_pred_class = tf.argmax(test_predictions, axis=1).numpy()

# Mengubah data_test dan test_pred_class menjadi DataFrame
df_test_predictions = pd.DataFrame({
    'text': data_test_preprocessed,
    'predicted_sentiment': test_pred_class
})

# Mengganti nilai numerik pada 'predicted_sentiment' dengan label yang sesuai
sentiment_labels = {0: 'positif', 1: 'netral', 2: 'negatif'}
df_test_predictions['predicted_sentiment'] = df_test_predictions['predicted_sentiment'].map(sentiment_labels)

df_test_predictions

#Fungsi untuk membuat pie chart
def plot_pie_chart(values, labels, title):
    plt.figure(figsize=(8, 6))
    plt.pie(values, labels=labels, autopct='%1.1f%%', startangle=140)
    plt.title(title)
    plt.show()

# Menghitung jumlah masing-masing sentimen pada data_train
sentimen_counts_test = df_test_predictions['predicted_sentiment'].value_counts().sort_index()
plot_pie_chart(sentimen_counts_test, ['positif', 'netral', 'negatif'], "Distribusi Sentimen pada Data Test (XLNet)")

from wordcloud import WordCloud
import matplotlib.pyplot as plt

def generate_wordcloud(text, title):
    # Generate the word cloud
    wordcloud = WordCloud(width=800, height=400, random_state=21, max_font_size=110, background_color='white').generate(text)

    # Plot the WordCloud image
    plt.figure(figsize=(10, 7))
    plt.imshow(wordcloud, interpolation="bilinear")
    plt.axis('off')
    plt.title(title)
    plt.show()

# Example: Assuming you have a column named 'text' in your DataFrame
text_data = ' '.join(df_test_predictions['text'].values)
generate_wordcloud(text_data, "Word Cloud from Test Data")